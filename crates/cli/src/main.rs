/// Redact secrets from a string
pub fn redact_secrets(input: &str) -> String {
    let secret_patterns = [
        r#""OPENAI_API_KEY":\s*"[^"]*""#,
        r#""openai_api_key":\s*"[^"]*""#,
        r#"OPENAI_API_KEY=[^\s]*"#,
        r#"openai_api_key=[^\s]*"#,
    ];
    
    let mut result = input.to_string();
    for pattern in &secret_patterns {
        let re = regex::Regex::new(pattern).unwrap();
        result = re.replace_all(&result, |caps: &regex::Captures| {
            let full_match = &caps[0];
            if full_match.contains(':') {
                // JSON format
                full_match.split(':').next().unwrap().to_string() + ": \"[REDACTED]\""
            } else {
                // Environment variable format
                full_match.split('=').next().unwrap().to_string() + "=[REDACTED]"
            }
        }).to_string();
    }
    result
}

use clap::{Parser, Subcommand};
use kernel::{AnalyzeResponse, Document};
use serde::{Deserialize, Serialize};
use std::fs;
use std::fs::File;
use std::io::{BufReader, BufWriter, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Duration;
use tokio;
use tokio::sync::Mutex;
use warp::Filter;

#[derive(Debug)]
struct MetricsError;

impl warp::reject::Reject for MetricsError {}

/// Metrics collected during pipeline execution
#[derive(Debug, Clone, Serialize, Deserialize)]
struct PipelineMetrics {
    /// Duration of scraping stage in milliseconds
    scrape_duration_ms: u128,
    /// Duration of analysis stage in milliseconds
    analyze_duration_ms: u128,
    /// Duration of summarization stage in milliseconds
    summarize_duration_ms: u128,
    /// Analyzer cache hit rate (0.0 to 1.0)
    analyzer_cache_hit_rate: f64,
    /// Summarizer cache hit (true/false)
    summarizer_cache_hit: bool,
    /// Input tokens used by summarizer
    input_tokens: usize,
    /// Output tokens generated by summarizer
    output_tokens: usize,
    /// Average pairwise cosine similarity among selected segments (redundancy indicator)
    avg_pairwise_cosine: f32,
}

/// CLI for the websearch pipeline
#[derive(Parser)]
#[clap(name = "websearch-cli")]
#[clap(about = "A CLI tool for web scraping, analysis, and summarization", long_about = None)]
struct Cli {
    #[clap(subcommand)]
    command: Commands,

    /// Bypass summary cache
    #[clap(long, global = true)]
    no_summary_cache: bool,
}

#[derive(Subcommand)]
enum Commands {
    /// Scrape a webpage
    Scrape {
        /// The URL to scrape
        url: String,
    },
    /// Analyze scraped content
    Analyze {
        /// Path to the scraped document
        path: String,

        /// Number of top segments to select
        #[clap(long, default_value = "10")]
        top_n: usize,

        /// MMR lambda parameter (0.0 = centroid only, 1.0 = diversity only)
        #[clap(long, default_value = "0.65")]
        mmr_lambda: f32,

        /// Enable reranking
        #[clap(long)]
        rerank: bool,

        /// Number of top segments to consider for reranking
        #[clap(long, default_value = "30")]
        rerank_top_m: usize,

        /// Batch size for inference
        #[clap(long, default_value = "8")]
        batch_size: usize,

        /// Maximum sequence length
        #[clap(long, default_value = "512")]
        max_seq_len: usize,

        /// Output file path (stdout if not provided)
        #[clap(short, long)]
        output: Option<String>,

        /// ONNX Provider to use, if not set it will try fetch one in current directory
        #[clap(long)]
        onnx_provider: Option<PathBuf>,
    },
    /// Summarize analyzed content
    Summarize {
        /// Path to the analyzed document (AnalyzeResponse JSON)
        #[clap(long)]
        analysis: String,

        /// Path to the original document (Document JSON)
        #[clap(long)]
        document: String,

        /// Style of summary to generate
        #[clap(long, default_value = "abstract_with_bullets")]
        style: String,

        /// Timeout for API requests (in milliseconds)
        #[clap(long, default_value = "30000")]
        timeout_ms: u64,

        /// Temperature for sampling
        #[clap(long, default_value = "0.2")]
        temperature: f32,

        /// Enable map-reduce summarization
        #[clap(long)]
        map_reduce: bool,

        /// Maximum context tokens before switching to map-reduce
        #[clap(long)]
        max_context_tokens: Option<usize>,

        /// Maximum tokens per map call
        #[clap(long)]
        map_group_tokens: Option<usize>,

        /// Target words for the reduce stage
        #[clap(long)]
        reduce_target_words: Option<usize>,

        /// Concurrency limit for map calls
        #[clap(long)]
        concurrency: Option<usize>,
    },
    /// Run the full pipeline
    Run {
        /// The URL to process
        url: String,

        /// Output directory (default: ./out)
        #[clap(long, default_value = "./out")]
        out_dir: String,

        /// Number of top segments to select
        #[clap(long, default_value = "10")]
        top_n: usize,

        /// MMR lambda parameter (0.0 = centroid only, 1.0 = diversity only)
        #[clap(long, default_value = "0.65")]
        mmr_lambda: f32,

        /// Enable reranking
        #[clap(long)]
        rerank: bool,

        /// Number of top segments to consider for reranking
        #[clap(long, default_value = "30")]
        rerank_top_m: usize,

        /// Style of summary to generate
        #[clap(long, default_value = "abstract_with_bullets")]
        style: String,

        /// Timeout for API requests (in milliseconds)
        #[clap(long, default_value = "30000")]
        timeout_ms: u64,

        /// Path to configuration file
        #[clap(long)]
        config: Option<String>,

        /// Enable map-reduce summarization
        #[clap(long)]
        map_reduce: bool,

        /// Maximum context tokens before switching to map-reduce
        #[clap(long)]
        max_context_tokens: Option<usize>,

        /// Maximum tokens per map call
        #[clap(long)]
        map_group_tokens: Option<usize>,

        /// Target words for the reduce stage
        #[clap(long)]
        reduce_target_words: Option<usize>,

        /// Concurrency limit for map calls
        #[clap(long)]
        concurrency: Option<usize>,

        /// ONNX Provider to use, if not set it will try fetch one in current directory
        #[clap(long)]
        onnx_provider: Option<PathBuf>,

        /// Output metrics as JSON to a file or stdout
        #[clap(long)]
        metrics_json: Option<String>,

        /// Port to serve metrics endpoint on (disabled if not specified)
        #[clap(long)]
        metrics_port: Option<u16>,
    },
    /// Cache management commands
    Cache {
        #[clap(subcommand)]
        subcommand: CacheCommands,
    },
    /// Health check command
    #[clap(alias = "healthcheck")]
    Health {},
    /// Print configuration
    Config {
        #[clap(subcommand)]
        subcommand: ConfigCommands,
    },
}

#[derive(Subcommand)]
enum ConfigCommands {
    /// Print the current configuration
    Print {
        /// Output in JSON format
        #[clap(long)]
        json: bool,
    },
}

#[derive(Subcommand)]
enum CacheCommands {
    /// Show analyzer cache statistics
    Stats,
    /// Clear the analyzer cache
    Clear,
    /// Show summarizer cache statistics
    SummaryStats,
    /// Clear the summarizer cache
    SummaryClear,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    utils::env::load_env();
    logger::init_logger();

    let cli = Cli::parse();

    let result = match cli.command {
        Commands::Scrape { url } => {
            println!("Scraping URL: {}", url);
            // TODO: Implement scraping
            Ok(())
        }
        Commands::Analyze {
            path,
            top_n,
            mmr_lambda,
            rerank,
            rerank_top_m,
            batch_size,
            max_seq_len,
            output,
            onnx_provider,
        } => {
            analyze_document(
                &path,
                top_n,
                mmr_lambda,
                rerank,
                rerank_top_m,
                batch_size,
                max_seq_len,
                onnx_provider,
                output,
            )
            .await
        }
        Commands::Summarize {
            analysis,
            document,
            style,
            timeout_ms,
            temperature,
            map_reduce,
            max_context_tokens,
            map_group_tokens,
            reduce_target_words,
            concurrency,
        } => {
            summarize_document(
                &analysis,
                &document,
                &style,
                timeout_ms,
                temperature,
                map_reduce,
                max_context_tokens,
                map_group_tokens,
                reduce_target_words,
                concurrency,
                cli.no_summary_cache,
            )
            .await
        }
        Commands::Run {
            url,
            out_dir,
            top_n,
            mmr_lambda,
            rerank,
            rerank_top_m,
            style,
            timeout_ms,
            config: _config,
            onnx_provider,
            map_reduce,
            max_context_tokens,
            map_group_tokens,
            reduce_target_words,
            concurrency,
            metrics_json,
            metrics_port,
        } => {
            run_pipeline(
                &url,
                &out_dir,
                top_n,
                mmr_lambda,
                rerank,
                rerank_top_m,
                &style,
                timeout_ms,
                onnx_provider,
                map_reduce,
                max_context_tokens,
                map_group_tokens,
                reduce_target_words,
                concurrency,
                cli.no_summary_cache,
                metrics_json,
                metrics_port,
            )
            .await
        }
        Commands::Cache { subcommand } => match subcommand {
            CacheCommands::Stats => {
                cache_stats().await
            }
            CacheCommands::Clear => {
                cache_clear().await
            }
            CacheCommands::SummaryStats => {
                summary_cache_stats().await
            }
            CacheCommands::SummaryClear => {
                summary_cache_clear().await
            }
        },
        Commands::Health {} => {
            health_check().await
        }
        Commands::Config { subcommand } => match subcommand {
            ConfigCommands::Print { json } => {
                config_print(json).await
            }
        }
    };

    // Handle the result
    match result {
        Ok(()) => {
            // Explicitly flush stdout to ensure all output is written before exit
            std::io::stdout().flush().ok();
            // Exit immediately to avoid cleanup issues
            std::process::exit(0);
        }
        Err(e) => {
            eprintln!("Error: {}", e);
            std::process::exit(1);
        }
    }
}

async fn analyze_document(
    path: &str,
    top_n: usize,
    mmr_lambda: f32,
    rerank: bool,
    rerank_top_m: usize,
    batch_size: usize,
    max_seq_len: usize,
    onnx_provider: Option<PathBuf>,
    output: Option<String>,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("Analyzing document: {}", path);

    // Load document
    let file = File::open(path)?;
    let reader = BufReader::new(file);
    let document: Document = serde_json::from_reader(reader)?;

    // Create analyzer configuration
    let mut config = analyzer::config::AnalyzerConfig {
        backend: "onnx".to_string(),
        model: analyzer::config::ModelConfig::HuggingFace(
            analyzer::config::HuggingFaceModelConfig {
                repo_id: "BAAI/bge-small-en-v1.5".to_string(),
                revision: "main".to_string(),
                files: vec![
                    "onnx/model.onnx".to_string(),
                    "tokenizer.json".to_string(),
                    "special_tokens_map.json".to_string(),
                ],
            },
        ),
        mmr_lambda,
        top_n,
        rerank,
        reranker: analyzer::config::RerankerConfig {
            enabled: rerank,
            top_m: rerank_top_m,
            ..Default::default()
        },
        allow_downloads: true,
        cache: analyzer::config::CacheConfig::default(),
        onnx_provider,
    };

    // If rerank is enabled, update the reranker model config
    if rerank {
        config.reranker.model =
            analyzer::config::ModelConfig::HuggingFace(analyzer::config::HuggingFaceModelConfig {
                repo_id: "BAAI/bge-reranker-base".to_string(),
                revision: "main".to_string(),
                files: vec!["onnx/model.onnx".to_string(), "tokenizer.json".to_string()],
            });
    }

    // Create analyzer
    let mut analyzer = analyzer::Analyzer::new(config).await?;

    // Log model info
    println!("Model ID: {}", analyzer.model_fingerprint());
    println!("Embedding dimension: 384");
    println!("Batch size: {}", batch_size);
    println!("Max sequence length: {}", max_seq_len);
    if rerank {
        println!("Rerank enabled: true");
        println!("Rerank top M: {}", rerank_top_m);
    } else {
        println!("Rerank enabled: false");
    }

    // Record start time
    let start_time = std::time::Instant::now();

    // Analyze document
    let response = analyzer.analyze(&document)?;

    // Record end time
    let duration = start_time.elapsed();
    println!("Document analysis completed in {:?}", duration);

    // Explicitly shutdown the analyzer to ensure proper cleanup before writing output
    analyzer.shutdown()?;

    // Write output
    if let Some(output_path) = output {
        let file = File::create(output_path)?;
        let writer = BufWriter::new(file);
        serde_json::to_writer_pretty(writer, &response)?;
    } else {
        serde_json::to_writer_pretty(std::io::stdout(), &response)?;
    }

    println!("Analysis complete");
    Ok(())
}

async fn summarize_document(
    analysis_path: &str,
    document_path: &str,
    style: &str,
    timeout_ms: u64,
    temperature: f32,
    map_reduce: bool,
    max_context_tokens: Option<usize>,
    map_group_tokens: Option<usize>,
    reduce_target_words: Option<usize>,
    concurrency: Option<usize>,
    no_summary_cache: bool,
) -> Result<(), Box<dyn std::error::Error>> {
    logger::init_logger(); // Setup logger
    println!("Summarizing document");
    println!("Analysis path: {}", analysis_path);
    println!("Document path: {}", document_path);

    // Load document
    let file = File::open(document_path)?;
    let reader = BufReader::new(file);
    let document: Document = serde_json::from_reader(reader)?;

    // Load analysis
    let file = File::open(analysis_path)?;
    let reader = BufReader::new(file);
    let analysis: AnalyzeResponse = serde_json::from_reader(reader)?;

    // Validate that the analysis matches the document
    if analysis.doc_id != document.doc_id {
        return Err("Document ID mismatch between document and analysis".into());
    }

    // Create summarizer configuration
    let style_enum = match style {
        "abstract_with_bullets" => summarizer::config::SummaryStyle::AbstractWithBullets,
        "tldr" => summarizer::config::SummaryStyle::TlDr,
        "extractive" => summarizer::config::SummaryStyle::Extractive,
        _ => summarizer::config::SummaryStyle::AbstractWithBullets, // Default
    };

    log::trace!(
        "Using OPENAI_BASE_URL={:?}",
        std::env::var("OPENAI_BASE_URL")
    );

    let mut config = summarizer::config::SummarizerConfig {
        base_url: std::env::var("OPENAI_BASE_URL").expect("Missing OPENAI_BASE_URL"),
        model: std::env::var("OPENAI_MODEL").expect("Missing OPENAI_MODEL"),
        timeout_ms,
        temperature,
        max_tokens: None, // Not configurable via CLI in this MVP
        style: style_enum,
        api_key: std::env::var("OPENAI_API_KEY").ok(),
        map_reduce: summarizer::config::MapReduceConfig {
            enabled: map_reduce,
            max_context_tokens: max_context_tokens.unwrap_or(6000),
            map_group_tokens: map_group_tokens.unwrap_or(1000),
            reduce_target_words: reduce_target_words.unwrap_or(200),
            concurrency: concurrency.unwrap_or(4),
        },
        cache: if no_summary_cache {
            let mut cache = summarizer::config::CacheConfig::default();
            cache.enabled = false;
            cache
        } else {
            summarizer::config::CacheConfig::default()
        },
    };

    // Override map-reduce config from environment if not provided via CLI
    if max_context_tokens.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_MAX_CONTEXT_TOKENS").map(|v| v.parse::<usize>().unwrap_or(6000)) {
            config.map_reduce.max_context_tokens = val;
        }
    }
    if map_group_tokens.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_MAP_GROUP_TOKENS").map(|v| v.parse::<usize>().unwrap_or(1000)) {
            config.map_reduce.map_group_tokens = val;
        }
    }
    if reduce_target_words.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_REDUCE_TARGET_WORDS").map(|v| v.parse::<usize>().unwrap_or(200)) {
            config.map_reduce.reduce_target_words = val;
        }
    }
    if concurrency.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_CONCURRENCY").map(|v| v.parse::<usize>().unwrap_or(4)) {
            config.map_reduce.concurrency = val;
        }
    }

    // Log config info
    println!("Model: {}", config.model);
    println!("Timeout: {} ms", config.timeout_ms);
    println!("Temperature: {}", config.temperature);
    println!("Style: {:?}", config.style);
    if no_summary_cache {
        println!("Summary cache: disabled (bypassed)");
    } else {
        println!("Summary cache: enabled");
    }

    // Create summarizer
    let summarizer = summarizer::Summarizer::new(config)?;

    // Log number of selected segments
    println!("Selected segments: {}", analysis.top_segments.len());

    // Record start time
    let start_time = std::time::Instant::now();

    // Summarize document
    let response = summarizer.summarize(&document, &analysis).await?;

    // Record end time
    let duration = start_time.elapsed();
    println!("Document summarization completed in {:?}", duration);

    // Write output to stdout
    serde_json::to_writer_pretty(std::io::stdout(), &response)?;

    println!("Summarization complete");
    Ok(())
}

/// Run the full pipeline: scrape → analyze → summarize
async fn run_pipeline(
    url: &str,
    out_dir: &str,
    top_n: usize,
    mmr_lambda: f32,
    rerank: bool,
    rerank_top_m: usize,
    style: &str,
    timeout_ms: u64,
    onnx_provider: Option<PathBuf>,
    map_reduce: bool,
    max_context_tokens: Option<usize>,
    map_group_tokens: Option<usize>,
    reduce_target_words: Option<usize>,
    concurrency: Option<usize>,
    no_summary_cache: bool,
    metrics_json: Option<String>,
    metrics_port: Option<u16>,
) -> Result<(), Box<dyn std::error::Error>> {
    use websearch::{scrape_webpage, scraped_to_document};

    println!("Running full pipeline for URL: {}", url);
    println!("Output directory: {}", out_dir);
    println!("Top N segments: {}", top_n);
    println!("MMR lambda: {}", mmr_lambda);
    if rerank {
        println!("Rerank enabled: true");
        println!("Rerank top M: {}", rerank_top_m);
    } else {
        println!("Rerank enabled: false");
    }
    println!("Summary style: {}", style);
    println!("API timeout: {} ms", timeout_ms);
    if no_summary_cache {
        println!("Summary cache: disabled (bypassed)");
    }

    // Create output directory if it doesn't exist
    fs::create_dir_all(out_dir)?;

    // Record start time
    let start_time = std::time::Instant::now();

    // Step 1: Scrape the webpage
    println!("\n--- Step 1: Scraping ---");
    let scrape_start = std::time::Instant::now();
    let scraped_data = scrape_webpage(url).await?;
    let scrape_duration = scrape_start.elapsed();
    println!("Scraping completed in {:?}", scrape_duration);

    // Convert scraped data to Document
    let document = scraped_to_document(url, &scraped_data);
    println!("Document ID: {}", document.doc_id);
    println!("Document title: {}", document.title);
    println!("Document language: {}", document.lang);
    println!("Number of segments: {}", document.segments.len());

    // Write document to file
    let document_path = Path::new(out_dir).join("document.json");
    let document_file = File::create(&document_path)?;
    let document_writer = BufWriter::new(document_file);
    serde_json::to_writer_pretty(document_writer, &document)?;
    println!("Document written to: {}", document_path.display());

    // Step 2: Analyze the document
    println!("\n--- Step 2: Analyzing ---");
    let analyze_start = std::time::Instant::now();

    // Create analyzer configuration
    let mut analyzer_config = analyzer::config::AnalyzerConfig {
        backend: "onnx".to_string(),
        model: analyzer::config::ModelConfig::HuggingFace(
            analyzer::config::HuggingFaceModelConfig {
                repo_id: "BAAI/bge-small-en-v1.5".to_string(),
                revision: "main".to_string(),
                files: vec![
                    "onnx/model.onnx".to_string(),
                    "tokenizer.json".to_string(),
                    "special_tokens_map.json".to_string(),
                ],
            },
        ),
        mmr_lambda,
        top_n,
        rerank,
        reranker: analyzer::config::RerankerConfig {
            enabled: rerank,
            top_m: rerank_top_m,
            ..Default::default()
        },
        allow_downloads: true,
        cache: analyzer::config::CacheConfig::default(),
        onnx_provider,
    };

    // If rerank is enabled, update the reranker model config
    if rerank {
        analyzer_config.reranker.model =
            analyzer::config::ModelConfig::HuggingFace(analyzer::config::HuggingFaceModelConfig {
                repo_id: "BAAI/bge-reranker-base".to_string(),
                revision: "main".to_string(),
                files: vec!["onnx/model.onnx".to_string(), "tokenizer.json".to_string()],
            });
    }

    // Create analyzer
    let mut analyzer = analyzer::Analyzer::new(analyzer_config).await?;

    // Log model info
    println!("Model ID: {}", analyzer.model_fingerprint());
    println!("Embedding dimension: 384");
    if rerank {
        println!("Rerank enabled: true");
        println!("Rerank top M: {}", rerank_top_m);
    }

    // Analyze document
    let analysis_response = analyzer.analyze(&document)?;
    let analyze_duration = analyze_start.elapsed();
    println!("Analysis completed in {:?}", analyze_duration);
    println!(
        "Selected segments: {}",
        analysis_response.top_segments.len()
    );

    // Get analyzer cache stats
    let (_analyzer_hits, _analyzer_misses, analyzer_hit_rate) = analyzer.cache().get_hit_miss_stats();

    // Explicitly shutdown the analyzer to ensure proper cleanup
    analyzer.shutdown()?;

    // Write analysis to file
    let analysis_path = Path::new(out_dir).join("analysis.json");
    let analysis_file = File::create(&analysis_path)?;
    let analysis_writer = BufWriter::new(analysis_file);
    serde_json::to_writer_pretty(analysis_writer, &analysis_response)?;
    println!("Analysis written to: {}", analysis_path.display());

    // Step 3: Summarize the document
    println!("\n--- Step 3: Summarizing ---");
    let summarize_start = std::time::Instant::now();

    // Create summarizer configuration
    let style_enum = match style {
        "abstract_with_bullets" => summarizer::config::SummaryStyle::AbstractWithBullets,
        "tldr" => summarizer::config::SummaryStyle::TlDr,
        "extractive" => summarizer::config::SummaryStyle::Extractive,
        _ => summarizer::config::SummaryStyle::AbstractWithBullets, // Default
    };

    let mut summarizer_config = summarizer::config::SummarizerConfig {
        base_url: std::env::var("OPENAI_BASE_URL").expect("Missing OPENAI_BASE_URL key"),
        model: std::env::var("OPENAI_MODEL").expect("Missing OPENAI_MODEL key"),
        timeout_ms,
        temperature: 0.2, // Default temperature
        max_tokens: None, // Not configurable via CLI in this MVP
        style: style_enum,
        api_key: std::env::var("OPENAI_API_KEY").ok(),
        map_reduce: summarizer::config::MapReduceConfig {
            enabled: map_reduce,
            max_context_tokens: max_context_tokens.unwrap_or(6000),
            map_group_tokens: map_group_tokens.unwrap_or(1000),
            reduce_target_words: reduce_target_words.unwrap_or(200),
            concurrency: concurrency.unwrap_or(4),
        },
        cache: if no_summary_cache {
            let mut cache = summarizer::config::CacheConfig::default();
            cache.enabled = false;
            cache
        } else {
            summarizer::config::CacheConfig::default()
        },
    };

    // Override map-reduce config from environment if not provided via CLI
    if max_context_tokens.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_MAX_CONTEXT_TOKENS").map(|v| v.parse::<usize>().unwrap_or(6000)) {
            summarizer_config.map_reduce.max_context_tokens = val;
        }
    }
    if map_group_tokens.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_MAP_GROUP_TOKENS").map(|v| v.parse::<usize>().unwrap_or(1000)) {
            summarizer_config.map_reduce.map_group_tokens = val;
        }
    }
    if reduce_target_words.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_REDUCE_TARGET_WORDS").map(|v| v.parse::<usize>().unwrap_or(200)) {
            summarizer_config.map_reduce.reduce_target_words = val;
        }
    }
    if concurrency.is_none() {
        if let Ok(val) = std::env::var("MAP_REDUCE_CONCURRENCY").map(|v| v.parse::<usize>().unwrap_or(4)) {
            summarizer_config.map_reduce.concurrency = val;
        }
    }

    // Log config info
    println!("Model: {}", summarizer_config.model);
    println!("Timeout: {} ms", summarizer_config.timeout_ms);
    println!("Style: {:?}", summarizer_config.style);

    // Create summarizer
    let summarizer = summarizer::Summarizer::new(summarizer_config)?;

    // Summarize document (with fallback on error)
    let summary_response = match summarizer.summarize(&document, &analysis_response).await {
        Ok(response) => response,
        Err(e) => {
            println!(
                "Warning: Summarization failed with error: {}. Using extractive fallback.",
                e
            );
            // Create a fallback summary response
            kernel::SummarizeResponse {
                summary_text: "Summary generation failed. Using extractive fallback.".to_string(),
                bullets: None,
                citations: None,
                guardrails: Some(kernel::GuardrailsInfo {
                    filtered: true,
                    reason: Some(format!("API error: {}", e)),
                }),
                metrics: kernel::SummarizationMetrics {
                    processing_time_ms: summarize_start.elapsed().as_millis() as u64,
                    input_tokens: 0,
                    output_tokens: 0,
                    cache_hit: false,
                    mode: "extractive".to_string(),
                    prompt_version: "v1".to_string(),
                },
            }
        }
    };

    let summarize_duration = summarize_start.elapsed();
    println!("Summarization completed in {:?}", summarize_duration);

    // Write summary to file
    let summary_path = Path::new(out_dir).join("summary.json");
    let summary_file = File::create(&summary_path)?;
    let summary_writer = BufWriter::new(summary_file);
    serde_json::to_writer_pretty(summary_writer, &summary_response)?;
    println!("Summary written to: {}", summary_path.display());

    // Print final summary
    println!("\n--- Summary ---");
    println!("{}", summary_response.summary_text);
    if let Some(bullets) = &summary_response.bullets {
        println!("\nKey Points:");
        for (i, bullet) in bullets.iter().enumerate() {
            println!("{}. {}", i + 1, bullet);
        }
    }

    // Print cache hit info if available
    if summary_response.metrics.cache_hit {
        println!("\n[Cache HIT] Response served from cache");
    } else {
        println!("\n[Cache MISS] Response generated fresh");
    }

    // Collect metrics
    let metrics = PipelineMetrics {
        scrape_duration_ms: scrape_duration.as_millis(),
        analyze_duration_ms: analyze_duration.as_millis(),
        summarize_duration_ms: summarize_duration.as_millis(),
        analyzer_cache_hit_rate: analyzer_hit_rate,
        summarizer_cache_hit: summary_response.metrics.cache_hit,
        input_tokens: summary_response.metrics.input_tokens,
        output_tokens: summary_response.metrics.output_tokens,
        avg_pairwise_cosine: analysis_response.metrics.avg_pairwise_cosine,
    };

    // Print metrics summary
    println!("\n--- Metrics Summary ---");
    println!("Scraping time: {} ms", metrics.scrape_duration_ms);
    println!("Analysis time: {} ms", metrics.analyze_duration_ms);
    println!("Summarization time: {} ms", metrics.summarize_duration_ms);
    println!("Analyzer cache hit rate: {:.2}%", metrics.analyzer_cache_hit_rate * 100.0);
    println!("Summarizer cache hit: {}", if metrics.summarizer_cache_hit { "Yes" } else { "No" });
    if metrics.input_tokens > 0 {
        println!("Input tokens: {}", metrics.input_tokens);
    }
    if metrics.output_tokens > 0 {
        println!("Output tokens: {}", metrics.output_tokens);
    }
    println!("Average pairwise cosine similarity: {:.4}", metrics.avg_pairwise_cosine);

    // Output metrics as JSON if requested
    if let Some(metrics_json_path) = metrics_json {
        if metrics_json_path == "-" {
            // Output to stdout
            serde_json::to_writer_pretty(std::io::stdout(), &metrics)?;
            println!(); // Add a newline after JSON output
        } else {
            // Output to file
            let metrics_file = File::create(&metrics_json_path)?;
            let metrics_writer = BufWriter::new(metrics_file);
            serde_json::to_writer_pretty(metrics_writer, &metrics)?;
            println!("Metrics written to: {}", metrics_json_path);
        }
    }

    // Start metrics server if requested
    if let Some(port) = metrics_port {
        println!("\nStarting metrics server on port {}", port);
        
        // Create a shared metrics object
        let shared_metrics = Arc::new(Mutex::new(metrics.clone()));
        
        // Create a route to serve metrics
        let metrics_route = warp::path("metrics")
            .and_then(move || {
                let metrics_clone = shared_metrics.clone();
                async move {
                    let metrics = metrics_clone.lock().await;
                    match serde_json::to_string_pretty(&*metrics) {
                        Ok(json) => Ok(warp::reply::json(&json)),
                        Err(_) => Err(warp::reject::custom(MetricsError))
                    }
                }
            });
        
        // Start the server in a background task
        tokio::spawn(async move {
            warp::serve(metrics_route)
                .run(([127, 0, 0, 1], port))
                .await;
        });
        
        // Give the server a moment to start
        tokio::time::sleep(Duration::from_millis(100)).await;
        println!("Metrics server started. Access metrics at http://localhost:{}/metrics", port);
    }

    // Print completion info
    let total_duration = start_time.elapsed();
    println!("\n--- Pipeline completed in {:?} ---", total_duration);
    println!("Output files:");
    println!("  Document:  {}", document_path.display());
    println!("  Analysis:  {}", analysis_path.display());
    println!("  Summary:   {}", summary_path.display());

    // If metrics server is running, wait a bit before exiting to allow access
    if metrics_port.is_some() {
        println!("\nPress Ctrl+C to exit");
        // Sleep for a short time to allow the metrics server to be accessed
        tokio::time::sleep(Duration::from_millis(500)).await;
    }

    Ok(())
}

/// Show cache statistics
async fn cache_stats() -> Result<(), Box<dyn std::error::Error>> {
    // Create default analyzer config to get cache path
    let config = analyzer::config::AnalyzerConfig::new();

    // Initialize cache
    let cache = analyzer::cache::EmbeddingCache::new(config.cache)?;

    // Get stats
    let stats = cache.get_stats()?;

    println!("Analyzer Cache Statistics:");
    println!("  Entry Count: {}", stats.entry_count);
    println!("  Estimated Size: {} bytes", stats.total_size_bytes);

    Ok(())
}

/// Clear the cache
async fn cache_clear() -> Result<(), Box<dyn std::error::Error>> {
    // Create default analyzer config to get cache path
    let config = analyzer::config::AnalyzerConfig::new();

    // Initialize cache
    let cache = analyzer::cache::EmbeddingCache::new(config.cache)?;

    // Clear cache
    cache.clear()?;

    println!("Analyzer cache cleared successfully");

    Ok(())
}

/// Show summarizer cache statistics
async fn summary_cache_stats() -> Result<(), Box<dyn std::error::Error>> {
    // Create default summarizer config to get cache path
    let config = summarizer::config::SummarizerConfig::new();

    // Initialize cache
    let cache = summarizer::cache::SummarizerCache::new(config.cache)?;

    // Get stats
    let stats = cache.get_stats()?;

    println!("Summarizer Cache Statistics:");
    println!("  Entry Count: {}", stats.entry_count);
    println!("  Estimated Size: {} bytes", stats.total_size_bytes);

    Ok(())
}

/// Clear the summarizer cache
async fn summary_cache_clear() -> Result<(), Box<dyn std::error::Error>> {
    // Create default summarizer config to get cache path
    let config = summarizer::config::SummarizerConfig::new();

    // Initialize cache
    let cache = summarizer::cache::SummarizerCache::new(config.cache)?;

    // Clear cache
    cache.clear()?;

    println!("Summarizer cache cleared successfully");

    Ok(())
}

/// Health check command
async fn health_check() -> Result<(), Box<dyn std::error::Error>> {
    println!("Running health check...");
    
    // Load environment
    utils::env::load_env();
    
    // Report active sources
    println!("\n--- Configuration Sources ---");
    if let Ok(env_file) = std::env::var("ENV_FILE") {
        println!("Environment file: {}", env_file);
    } else {
        println!("Environment file: .env (default)");
    }
    
    // Show environment variables (redacted)
    println!("Environment variables:");
    let env_vars = [
        "OPENAI_BASE_URL",
        "OPENAI_MODEL",
        "OPENAI_API_KEY",
        "ANALYZER_ALLOW_DOWNLOADS",
        "MAP_REDUCE_ENABLED",
        "MAP_REDUCE_MAX_CONTEXT_TOKENS",
        "MAP_REDUCE_MAP_GROUP_TOKENS",
        "MAP_REDUCE_REDUCE_TARGET_WORDS",
        "MAP_REDUCE_CONCURRENCY",
        "SUMMARY_CACHE_ENABLED",
        "SUMMARY_CACHE_PATH",
        "SUMMARY_CACHE_TTL_DAYS",
    ];
    
    for var in &env_vars {
        if let Ok(value) = std::env::var(var) {
            let display_value = if var.contains("API_KEY") {
                "[REDACTED]".to_string()
            } else {
                value
            };
            println!("  {}: {}", var, display_value);
        }
    }
    
    // Analyzer checks
    println!("\n--- Analyzer Checks ---");
    
    // Create analyzer configuration
    let analyzer_config = analyzer::config::AnalyzerConfig::new();
    let analyzer_config_clone = analyzer_config.clone();
    
    // Check model files
    println!("Checking embedder model files...");
    match analyzer::model::resolve_model(&analyzer_config).await {
        Ok(resolved_model) => {
            println!("  ✓ Embedder model resolved: {}", resolved_model.fingerprint);
            println!("  ✓ Model files:");
            for path in &resolved_model.file_paths {
                if path.exists() {
                    println!("    ✓ {}", path.display());
                } else {
                    println!("    ✗ {} (missing)", path.display());
                    std::process::exit(1);
                }
            }
            
            // Try to create ONNX session
            println!("  Creating ONNX session...");
            match analyzer::Analyzer::new(analyzer_config_clone).await {
                Ok(analyzer) => {
                    println!("  ✓ ONNX session created successfully");
                    println!("  ✓ Model fingerprint: {}", analyzer.model_fingerprint());
                    
                    // Shutdown analyzer
                    if let Err(e) = analyzer.shutdown() {
                        println!("  ⚠ Warning: Error shutting down analyzer: {}", e);
                    }
                }
                Err(e) => {
                    println!("  ✗ Failed to create ONNX session: {}", e);
                    std::process::exit(1);
                }
            }
        }
        Err(e) => {
            println!("  ✗ Failed to resolve embedder model: {}", e);
            std::process::exit(1);
        }
    }
    
    // Check reranker if enabled
    let analyzer_config_for_reranker = analyzer::config::AnalyzerConfig::new();
    if analyzer_config_for_reranker.rerank && analyzer_config_for_reranker.reranker.enabled {
        println!("Checking reranker model files...");
        let mut reranker_config = analyzer_config_for_reranker.clone();
        reranker_config.model = analyzer_config_for_reranker.reranker.model.clone();
        
        match analyzer::model::resolve_model(&reranker_config).await {
            Ok(resolved_model) => {
                println!("  ✓ Reranker model resolved: {}", resolved_model.fingerprint);
                println!("  ✓ Model files:");
                for path in &resolved_model.file_paths {
                    if path.exists() {
                        println!("    ✓ {}", path.display());
                    } else {
                        println!("    ✗ {} (missing)", path.display());
                        std::process::exit(1);
                    }
                }
            }
            Err(e) => {
                println!("  ✗ Failed to resolve reranker model: {}", e);
                std::process::exit(1);
            }
        }
    }
    
    // Cache checks
    println!("\n--- Cache Checks ---");
    
    // Analyzer cache
    let analyzer_config_for_cache = analyzer::config::AnalyzerConfig::new();
    println!("Checking analyzer cache...");
    match analyzer::cache::EmbeddingCache::new(analyzer_config_for_cache.cache.clone()) {
        Ok(cache) => {
            match cache.get_stats() {
                Ok(stats) => {
                    println!("  ✓ Analyzer cache accessible");
                    println!("  ✓ Entry count: {}", stats.entry_count);
                    println!("  ✓ Estimated size: {} bytes", stats.total_size_bytes);
                }
                Err(e) => {
                    println!("  ⚠ Warning: Could not get analyzer cache stats: {}", e);
                }
            }
        }
        Err(e) => {
            println!("  ✗ Failed to initialize analyzer cache: {}", e);
            std::process::exit(1);
        }
    }
    
    // Summarizer cache
    println!("Checking summarizer cache...");
    let summarizer_config = summarizer::config::SummarizerConfig::new();
    match summarizer::cache::SummarizerCache::new(summarizer_config.cache.clone()) {
        Ok(cache) => {
            match cache.get_stats() {
                Ok(stats) => {
                    println!("  ✓ Summarizer cache accessible");
                    println!("  ✓ Entry count: {}", stats.entry_count);
                    println!("  ✓ Estimated size: {} bytes", stats.total_size_bytes);
                }
                Err(e) => {
                    println!("  ⚠ Warning: Could not get summarizer cache stats: {}", e);
                }
            }
        }
        Err(e) => {
            println!("  ✗ Failed to initialize summarizer cache: {}", e);
            std::process::exit(1);
        }
    }
    
    // Summarizer checks
    println!("\n--- Summarizer Checks ---");
    
    // Check base URL reachability
    let base_url = std::env::var("OPENAI_BASE_URL").unwrap_or_else(|_| "https://api.openai.com/v1".to_string());
    println!("Checking summarizer base URL: {}", base_url);
    
    // Try to create a simple HTTP client and make a request to the base URL
    let client = reqwest::Client::new();
    let test_url = format!("{}/models", base_url.trim_end_matches('/'));
    
    match tokio::time::timeout(std::time::Duration::from_secs(5), client.get(&test_url).send()).await {
        Ok(Ok(response)) => {
            if response.status().is_success() {
                println!("  ✓ Base URL is reachable");
            } else {
                println!("  ⚠ Base URL responded with status: {}", response.status());
            }
        }
        Ok(Err(e)) => {
            println!("  ⚠ Warning: Could not reach base URL: {}", e);
        }
        Err(_) => {
            println!("  ⚠ Warning: Base URL check timed out");
        }
    }
    
    // Check API key
    match std::env::var("OPENAI_API_KEY") {
        Ok(api_key) if !api_key.is_empty() => {
            println!("  ✓ API key present");
        }
        _ => {
            println!("  ⚠ API key missing (summarizer will be DEGRADED)");
        }
    }
    
    println!("\n--- Health Check Complete ---");
    println!("Status: OK");
    Ok(())
}

/// Config print command
async fn config_print(json: bool) -> Result<(), Box<dyn std::error::Error>> {
    // Load environment
    utils::env::load_env();
    
    // Create configurations
    let analyzer_config = analyzer::config::AnalyzerConfig::new();
    let summarizer_config = summarizer::config::SummarizerConfig::new();
    
    if json {
        // Create a combined config struct for JSON output
        #[derive(Serialize)]
        struct CombinedConfig {
            analyzer: analyzer::config::AnalyzerConfig,
            summarizer: summarizer::config::SummarizerConfig,
        }
        
        let combined = CombinedConfig {
            analyzer: analyzer_config,
            summarizer: summarizer_config,
        };
        
        // Serialize to JSON and redact secrets
        let json_str = serde_json::to_string_pretty(&combined)?;
        let redacted = redact_secrets(&json_str);
        println!("{}", redacted);
    } else {
        // Human-readable format
        println!("=== Analyzer Configuration ===");
        println!("Backend: {}", analyzer_config.backend);
        println!("Model fingerprint: {}", analyzer_config.model_fingerprint());
        println!("MMR lambda: {}", analyzer_config.mmr_lambda);
        println!("Top N segments: {}", analyzer_config.top_n);
        println!("Rerank enabled: {}", analyzer_config.rerank);
        if analyzer_config.rerank {
            println!("Reranker top M: {}", analyzer_config.reranker.top_m);
            println!("Reranker model fingerprint: {}", 
                match &analyzer_config.reranker.model {
                    analyzer::config::ModelConfig::HuggingFace(hf_config) => {
                        format!("{}@{}", hf_config.repo_id, hf_config.revision)
                    }
                    analyzer::config::ModelConfig::Local(local_config) => {
                        format!("local:{}", local_config.model_dir)
                    }
                }
            );
        }
        println!("Allow downloads: {}", analyzer_config.allow_downloads);
        println!("Cache enabled: {}", analyzer_config.cache.enabled);
        println!("Cache path: {}", analyzer_config.cache.path);
        if let Some(ttl) = analyzer_config.cache.ttl_days {
            println!("Cache TTL: {} days", ttl);
        }
        
        println!("\n=== Summarizer Configuration ===");
        println!("Base URL: {}", summarizer_config.base_url);
        println!("Model: {}", summarizer_config.model);
        println!("Timeout: {} ms", summarizer_config.timeout_ms);
        println!("Temperature: {}", summarizer_config.temperature);
        println!("Style: {:?}", summarizer_config.style);
        match summarizer_config.style {
            summarizer::config::SummaryStyle::AbstractWithBullets => println!("  (Abstract summary with bullet points)"),
            summarizer::config::SummaryStyle::TlDr => println!("  (TL;DR style summary)"),
            summarizer::config::SummaryStyle::Extractive => println!("  (Extractive summary)"),
        }
        println!("API key present: {}", summarizer_config.api_key.is_some());
        println!("Map-reduce enabled: {}", summarizer_config.map_reduce.enabled);
        if summarizer_config.map_reduce.enabled {
            println!("Map-reduce max context tokens: {}", summarizer_config.map_reduce.max_context_tokens);
            println!("Map-reduce map group tokens: {}", summarizer_config.map_reduce.map_group_tokens);
            println!("Map-reduce reduce target words: {}", summarizer_config.map_reduce.reduce_target_words);
            println!("Map-reduce concurrency: {}", summarizer_config.map_reduce.concurrency);
        }
        println!("Cache enabled: {}", summarizer_config.cache.enabled);
        println!("Cache path: {}", summarizer_config.cache.path);
        println!("Cache TTL: {} days", summarizer_config.cache.ttl_days);
        
        // Environment variables that override config
        println!("\n=== Environment Variables ===");
        let env_vars = [
            ("OPENAI_BASE_URL", "Summarizer base URL"),
            ("OPENAI_MODEL", "Summarizer model"),
            ("OPENAI_API_KEY", "Summarizer API key"),
            ("ANALYZER_ALLOW_DOWNLOADS", "Allow model downloads"),
            ("MAP_REDUCE_ENABLED", "Enable map-reduce"),
            ("MAP_REDUCE_MAX_CONTEXT_TOKENS", "Map-reduce max context tokens"),
            ("MAP_REDUCE_MAP_GROUP_TOKENS", "Map-reduce map group tokens"),
            ("MAP_REDUCE_REDUCE_TARGET_WORDS", "Map-reduce reduce target words"),
            ("MAP_REDUCE_CONCURRENCY", "Map-reduce concurrency"),
            ("SUMMARY_CACHE_ENABLED", "Enable summarizer cache"),
            ("SUMMARY_CACHE_PATH", "Summarizer cache path"),
            ("SUMMARY_CACHE_TTL_DAYS", "Summarizer cache TTL"),
        ];
        
        for (var, description) in &env_vars {
            if let Ok(value) = std::env::var(var) {
                let display_value = if var.contains("API_KEY") {
                    "[REDACTED]".to_string()
                } else {
                    value
                };
                println!("  {}: {} = {}", var, description, display_value);
            }
        }
    }
    
    Ok(())
}